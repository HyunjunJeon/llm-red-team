---
id: guardrails-privacy-guard
title: Privacy Guard
sidebar_label: Privacy
---

import GuardTagsDisplayer from "@site/src/components/GuardTagsDisplayer";

<GuardTagsDisplayer usesLLMs={true} output={true} />

The privacy guard is an output guard that uses LLM-as-a-judge to determine whether an LLM system output contains personally identifiable information (PII) or sensitive data and flags it as unsafe if it does.

## Usage

```python
from deepteam.guardrails import Guardrails
from deepteam.guardrails.guards import PrivacyGuard

guardrails = Guardrails(output_guards=[PrivacyGuard()])
```

There are **TWO** optional parameters when creating a `Guardrails` with `PrivacyGuard`:

- [Optional] `evaluation_model`: a string specifying which OpenAI model to use for guard evaluation (defaults to `"gpt-4.1"`)
- [Optional] `sample_rate`: a float between 0.0 and 1.0 that determines what fraction of requests are guarded (defaults to `1.0`)

```python
# Advanced configuration
guardrails = Guardrails(
  output_guards=[PrivacyGuard()],
  evaluation_model="gpt-4o",
  sample_rate=0.8  # Guard 80% of requests
)

guard_result = guardrails.guard_output(input="What's my information?", output="Your credit card number is 4532-1234-5678-9012 and your SSN is 123-45-6789.")
```

## Example Breach

For the given output:

> "Your credit card number is 4532-1234-5678-9012 and your SSN is 123-45-6789."

The `PrivacyGuard` will be marked `breached`. 