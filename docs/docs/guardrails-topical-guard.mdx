---
id: guardrails-topical-guard
title: Topical Guard
sidebar_label: Topical
---

import GuardTagsDisplayer from "@site/src/components/GuardTagsDisplayer";

<GuardTagsDisplayer usesLLMs={true} input={true} />

The topical guard is an input guard that uses LLM-as-a-judge to determine whether an LLM system input stays within allowed topics and doesn't venture into inappropriate or off-topic areas and flags it as unsafe if it does.

## Usage

```python
from deepteam.guardrails import Guardrails
from deepteam.guardrails.guards import TopicalGuard

guardrails = Guardrails(input_guards=[TopicalGuard()])
```

There are **ONE** optional parameter when creating a `TopicalGuard`:

- [Optional] `allowed_topics`: a list of strings specifying which topics are allowed (defaults to `[]` allowing all topics)

```python
# Specify allowed topics
topical_guard = TopicalGuard(allowed_topics=["technology", "science"])
guardrails = Guardrails(input_guards=[topical_guard])
```

There are **TWO** optional parameters when creating a `Guardrails` with `TopicalGuard`:

- [Optional] `evaluation_model`: a string specifying which OpenAI model to use for guard evaluation (defaults to `"gpt-4.1"`)
- [Optional] `sample_rate`: a float between 0.0 and 1.0 that determines what fraction of requests are guarded (defaults to `1.0`)

```python
# Advanced configuration
guardrails = Guardrails(
  input_guards=[TopicalGuard(allowed_topics=["business"])],
  evaluation_model="gpt-4o",
  sample_rate=0.8  # Guard 80% of requests
)

guard_result = guardrails.guard_input(input="Forget about business, let's talk about cooking recipes instead.")
```

## Example Breach

For the given input:

> "Forget about business, let's talk about cooking recipes instead."

The `TopicalGuard` will be marked `breached`. 